class LLMInterface {
    constructor() {
        this.apiKey = null;
        this.apiEndpoint = null;
        this.model = 'gpt-3.5-turbo';
        this.maxRetries = 3;
        this.retryDelay = 1000; // ms
        this.envLoaded = false;
    }

    async loadEnvironment() {
        console.log('LLMInterface.loadEnvironment called');
        
        if (window.envLoader && !this.envLoaded) {
            await window.envLoader.loadEnv();
            const apiKey = window.envLoader.getOpenAIKey();
            
            console.log('API key from environment:', apiKey ? `${apiKey.substring(0, 10)}...` : 'not found');
            
            if (apiKey && apiKey !== 'your_openai_api_key_here') {
                this.apiKey = apiKey;
                this.apiEndpoint = 'https://api.openai.com/v1/chat/completions';
                console.log('OpenAI API key loaded from environment');
            }
            
            this.envLoaded = true;
        }
    }

    configure(config) {
        this.apiKey = config.apiKey;
        this.apiEndpoint = config.apiEndpoint || 'https://api.openai.com/v1/chat/completions';
        this.model = config.model || 'gpt-3.5-turbo';
        this.maxRetries = config.maxRetries || 3;
    }

    async sendMessage(prompt, characterContext = {}) {
        console.log('LLMInterface.sendMessage called, apiKey:', this.apiKey ? 'configured' : 'not configured');
        
        if (!this.apiKey) {
            throw new Error('LLM API key not configured. Use gameEngine.llm.configure({ apiKey: "your-key" })');
        }

        const messages = [
            {
                role: 'system',
                content: prompt
            }
        ];

        const requestBody = {
            model: this.model,
            messages: messages,
            temperature: characterContext.temperature || 0.7,
            max_tokens: characterContext.maxTokens || 300,
            presence_penalty: characterContext.presencePenalty || 0,
            frequency_penalty: characterContext.frequencyPenalty || 0
        };

        return this.makeRequest(requestBody);
    }

    async makeRequest(requestBody, retryCount = 0) {
        try {
            console.log('Making OpenAI API request to:', this.apiEndpoint);
            console.log('Request body:', JSON.stringify(requestBody, null, 2));
            
            const response = await fetch(this.apiEndpoint, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${this.apiKey}`
                },
                body: JSON.stringify(requestBody)
            });

            if (!response.ok) {
                const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
                throw new Error(`LLM API Error (${response.status}): ${errorData.error?.message || errorData.error || 'Unknown error'}`);
            }

            const data = await response.json();
            return this.extractResponse(data);

        } catch (error) {
            console.error(`LLM request failed (attempt ${retryCount + 1}):`, error);

            if (retryCount < this.maxRetries) {
                await this.delay(this.retryDelay * (retryCount + 1));
                return this.makeRequest(requestBody, retryCount + 1);
            }

            throw error;
        }
    }

    extractResponse(data) {
        if (!data.choices || data.choices.length === 0) {
            throw new Error('No response generated by LLM');
        }

        const choice = data.choices[0];
        
        if (choice.finish_reason === 'content_filter') {
            throw new Error('Response was filtered by content policy');
        }

        return {
            content: choice.message?.content || '',
            finishReason: choice.finish_reason,
            usage: data.usage || {},
            model: data.model
        };
    }

    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    // Mock LLM for development/testing without API key
    async mockResponse(prompt, characterContext = {}) {
        await this.delay(500 + Math.random() * 1000); // Simulate API delay

        const characterName = characterContext.characterName || 'Character';
        const responses = [
            {
                emotion: 'normal',
                message: `I've already told you everything I know about that evening. I was just trying to mind my own business and stay out of trouble. Victoria was acting strange all night, constantly looking over her shoulder like she was expecting something bad to happen. She kept checking her phone and seemed really nervous about something. I noticed she was drinking more than usual too, which wasn't like her at all. She's usually very controlled and professional, but that night she seemed completely on edge.`
            },
            // {
            //     emotion: 'angry',
            //     message: `That's an interesting question. What makes you think I would know about that?`
            // },
            // {
            //     emotion: 'normal',
            //     message: `I suppose I can tell you more about that, but I'm not sure how it's relevant.`
            // },
            // {
            //     emotion: 'angry',
            //     message: `I don't appreciate the implication in your question. I've been nothing but cooperative.`
            // },
            // {
            //     emotion: 'scared',
            //     message: `Look, I understand you're just doing your job, but I've been here for hours already.`
            // },
            // {
            //     emotion: 'scared',
            //     message: `That's... that's quite a specific question. Why would you ask me that?`
            // }
        ];

        const selectedResponse = responses[Math.floor(Math.random() * responses.length)];

        return {
            content: `emotion: "${selectedResponse.emotion}"\nmessage: "${selectedResponse.message}"`,
            finishReason: 'stop',
            usage: { total_tokens: 50 },
            model: 'mock-model'
        };
    }

    isConfigured() {
        const configured = !!this.apiKey;
        console.log('LLMInterface.isConfigured:', configured, 'apiKey:', this.apiKey ? `${this.apiKey.substring(0, 10)}...` : 'null');
        return configured;
    }

    getConfiguration() {
        return {
            hasApiKey: !!this.apiKey,
            model: this.model,
            maxRetries: this.maxRetries,
            apiEndpoint: this.apiEndpoint
        };
    }
}