class LLMInterface {
    constructor() {
        this.apiKey = null;
        this.apiEndpoint = null;
        this.model = 'gpt-3.5-turbo';
        this.maxRetries = 3;
        this.retryDelay = 1000; // ms
        this.envLoaded = false;
    }

    async loadEnvironment() {
        console.log('LLMInterface.loadEnvironment called - using Vercel API, no client-side API key needed');
        this.envLoaded = true;
    }

    configure(config) {
        this.apiKey = config.apiKey;
        this.apiEndpoint = config.apiEndpoint || 'https://api.openai.com/v1/chat/completions';
        this.model = config.model || 'gpt-3.5-turbo';
        this.maxRetries = config.maxRetries || 3;
    }

    async sendMessage(prompt, characterContext = {}) {
        console.log('LLMInterface.sendMessage called - using Vercel API');

        const messages = [
            {
                role: 'system',
                content: prompt
            }
        ];

        const requestBody = {
            model: this.model,
            messages: messages,
            temperature: characterContext.temperature || 0.7,
            max_tokens: characterContext.maxTokens || 300
        };

        return this.makeRequest(requestBody);
    }

    async makeRequest(requestBody, retryCount = 0) {
        try {
            const vercelApiUrl = '/api/chat';
            console.log('Making request to Vercel API:', vercelApiUrl);
            console.log('Request body:', JSON.stringify(requestBody, null, 2));
            
            const response = await fetch(vercelApiUrl, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify(requestBody)
            });

            if (!response.ok) {
                // If API route not found (404), fall back to mock for local development
                if (response.status === 404) {
                    console.log('Vercel API not available, using mock response for local development');
                    return this.mockResponse('', {});
                }
                
                const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
                throw new Error(`LLM API Error (${response.status}): ${errorData.error?.message || errorData.error || 'Unknown error'}`);
            }

            const data = await response.json();
            return this.extractResponse(data);

        } catch (error) {
            console.error(`LLM request failed (attempt ${retryCount + 1}):`, error);

            // If it's a network error (e.g., API route doesn't exist), fall back to mock
            if (error.message.includes('fetch') || error.message.includes('Failed to fetch')) {
                console.log('Network error, falling back to mock response for local development');
                return this.mockResponse('', {});
            }

            if (retryCount < this.maxRetries) {
                await this.delay(this.retryDelay * (retryCount + 1));
                return this.makeRequest(requestBody, retryCount + 1);
            }

            throw error;
        }
    }

    extractResponse(data) {
        if (!data.choices || data.choices.length === 0) {
            throw new Error('No response generated by LLM');
        }

        const choice = data.choices[0];
        
        if (choice.finish_reason === 'content_filter') {
            throw new Error('Response was filtered by content policy');
        }

        return {
            content: choice.message?.content || '',
            finishReason: choice.finish_reason,
            usage: data.usage || {},
            model: data.model
        };
    }

    delay(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    // Mock LLM for development/testing without API key
    async mockResponse(prompt, characterContext = {}) {
        await this.delay(500 + Math.random() * 1000); // Simulate API delay

        const characterName = characterContext.characterName || 'Character';
        const responses = [
            {
                emotion: 'normal',
                message: `I've already told you everything I know about that evening. I was just trying to mind my own business and stay out of trouble. Victoria was acting strange all night, constantly looking over her shoulder like she was expecting something bad to happen. She kept checking her phone and seemed really nervous about something. I noticed she was drinking more than usual too, which wasn't like her at all. She's usually very controlled and professional, but that night she seemed completely on edge.`
            },
            // {
            //     emotion: 'angry',
            //     message: `That's an interesting question. What makes you think I would know about that?`
            // },
            // {
            //     emotion: 'normal',
            //     message: `I suppose I can tell you more about that, but I'm not sure how it's relevant.`
            // },
            // {
            //     emotion: 'angry',
            //     message: `I don't appreciate the implication in your question. I've been nothing but cooperative.`
            // },
            // {
            //     emotion: 'scared',
            //     message: `Look, I understand you're just doing your job, but I've been here for hours already.`
            // },
            // {
            //     emotion: 'scared',
            //     message: `That's... that's quite a specific question. Why would you ask me that?`
            // }
        ];

        const selectedResponse = responses[Math.floor(Math.random() * responses.length)];

        return {
            content: `emotion: "${selectedResponse.emotion}"\nmessage: "${selectedResponse.message}"`,
            finishReason: 'stop',
            usage: { total_tokens: 50 },
            model: 'mock-model'
        };
    }

    isConfigured() {
        // Always return true for Vercel deployment - API key is handled server-side
        console.log('LLMInterface.isConfigured: true (using Vercel API)');
        return true;
    }

    getConfiguration() {
        return {
            hasApiKey: !!this.apiKey,
            model: this.model,
            maxRetries: this.maxRetries,
            apiEndpoint: this.apiEndpoint
        };
    }
}